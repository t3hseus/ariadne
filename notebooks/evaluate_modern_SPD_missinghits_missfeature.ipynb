{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_265068/1187170015.py:13: DeprecationWarning: Importing clear_output from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import clear_output, display\n",
      "/tmp/ipykernel_265068/1187170015.py:13: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import clear_output, display\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "#import ariadne.graph_net.model\n",
    "import gin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from IPython.core.display import clear_output, display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "    \n",
    "from eval.event_evaluation import EventEvaluator\n",
    "from ariadne_v2.transformations import Compose, DropShort, DropTracksWithHoles, DropSpinningTracks, BakeStationValues, ConstraintsNormalize, FixStationsBMN, ToCylindrical\n",
    "from ariadne.utils.model import get_checkpoint_path, weights_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ariadne.utils.timer import CudaTimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import scripts.clean_cache\n",
    "\n",
    "#to clean cache if needed\n",
    "#scripts.clean_cache.clean_jit_cache('20w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tracknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#torch.set_num_threads(1)\n",
    "#torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "import faiss\n",
    "import onnxruntime as ort\n",
    "#ort.set_default_logger_severity(0)\n",
    "\n",
    "NUM_POINTS_TO_SEARCH = 1\n",
    "\n",
    "from ariadne_v2.inference import IModelLoader\n",
    "from ariadne.tracknet_v2.model import TrackNETv2\n",
    "class TrackNetModelLoader(IModelLoader):    \n",
    "    def __call__(self):\n",
    "        \n",
    "        tracknet_ckpt_path_dict = {'model_dir': '/lustre/home/user/d/drusov/ariadne_spd/ariadne/lightning_logs/TrackNETMissingHits',\n",
    "                                   'version': 'version_19', 'checkpoint': 'latest'}# version 10\n",
    "                                   #'version': 'OnlyRNN_alpha0.01_unbalanced', 'checkpoint': 'latest'}\n",
    "        path_to_tracknet_ckpt = get_checkpoint_path(**tracknet_ckpt_path_dict)\n",
    "\n",
    "        model_tr = weights_update(model=TrackNETInference(),\n",
    "                       checkpoint=torch.load(path_to_tracknet_ckpt, map_location=torch.device(DEVICE)))\n",
    "        model_tr.eval()\n",
    "        model_tr = model_tr.to(DEVICE)\n",
    "        for param in model_tr.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        model_tr = torch.jit.script(model_tr).to(DEVICE)\n",
    "        model_tr = torch.jit.optimize_for_inference(model_tr)\n",
    "        \"\"\"\n",
    "        path_to_tracknet_ckpt = 'torchscript_model.pt'\n",
    "        model_tr = torch.jit.load(path_to_tracknet_ckpt)\n",
    "        model_tr.eval()\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        torch.onnx.export(model_tr, \n",
    "                          dummy_tracks, \n",
    "                          \"tracknetv3.onnx\",\n",
    "                          export_params=True,\n",
    "                          opset_version=11,\n",
    "                          do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                          input_names = ['inputs'],   # the model's input names\n",
    "                          output_names = ['output'], # the model's output names\n",
    "                          dynamic_axes={'inputs' : {0 : 'batch_size'},    # variable length axes\n",
    "                                        'output' : {0 : 'batch_size'}})\n",
    "        \n",
    "        so = ort.SessionOptions()\n",
    "        so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "        providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "        providers = providers if DEVICE == 'cuda' else providers[1:]\n",
    "        model_tr = ort.InferenceSession('tracknetv3.onnx', sess_options=so, providers=providers)\n",
    "        \"\"\"\n",
    "        model_hash = {\n",
    "            \"tracknet_ckpt_path_dict\":path_to_tracknet_ckpt,\n",
    "            'gin':gin.config_str(), \n",
    "            'model': '%r' % model_tr,\n",
    "            'NUM_POINTS_TO_SEARCH':NUM_POINTS_TO_SEARCH\n",
    "        }\n",
    "        return model_hash, [model_tr]\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)\n",
    "import ariadne.transformations as trn\n",
    "\n",
    "tracknet_transformer = trn.Compose([\n",
    "    #trn.DropShort(num_stations=4),\n",
    "    #trn.DropTracksWithHoles(),\n",
    "    #trn.DropSpinningTracks(),\n",
    "    #trn.BakeStationValues(values=z_values),\n",
    "    #trn.ConstraintsNormalize(constraints=constraints),\n",
    "    #trn.ToCylindrical()\n",
    "    ]\n",
    ")\n",
    "\n",
    "def preprocess_one_event(event_df):\n",
    "    try:\n",
    "        tracknet_data = tracknet_transformer(event_df)\n",
    "    except AssertionError as err:\n",
    "        #print(\"ASS error %r\" % err)\n",
    "        return None\n",
    "    numpy_data = parse_df(tracknet_data)\n",
    "    return ((numpy_data, tracknet_data), 1)\n",
    "\n",
    "times = []\n",
    "\n",
    "res_1 = None\n",
    "\n",
    "from ariadne.graph_net.dataset import collate_fn\n",
    "\n",
    "def run_tracknet_eval(result_from_prev_step, model, n_neighbours=NUM_POINTS_TO_SEARCH):\n",
    "    numpy_data, event_df = result_from_prev_step[0]\n",
    "    preds = run_model(numpy_data, model)\n",
    "        \n",
    "    #ev_first, ev_drop1, ev_drop2, ev_model1, ev_model2, ev_search2, ev_filter2, ev_prolong2, ev_last, ev_dropd_1, ev_dropd_2 = cuda_events\n",
    "    #torch.cuda.synchronize()\n",
    "    #times['total'].append(ev_first.elapsed_time(ev_last))\n",
    "    #try:\n",
    "    #    times['drop'].append(ev_drop1.elapsed_time(ev_drop2))\n",
    "    #except:\n",
    "    #    pass    \n",
    "    #try:\n",
    "    #    times['dropd'].append(ev_dropd_1.elapsed_time(ev_dropd_2))\n",
    "    #except:\n",
    "    #    pass\n",
    "    #for i in range(34):\n",
    "    #    try:\n",
    "    #        times['model'].append(ev_model1[i].elapsed_time(ev_model2[i]))\n",
    "    #    except:\n",
    "    #        break\n",
    "    #    times['search'].append(ev_model2[i].elapsed_time(ev_search2[i]))\n",
    "    #    times['filter'].append(ev_search2[i].elapsed_time(ev_filter2[i]))\n",
    "    #    times['prolong'].append(ev_filter2[i].elapsed_time(ev_prolong2[i]))\n",
    "    \n",
    "    result = build_df(preds, event_df)\n",
    "    for col in [f'hit_id_{i}' for i in range(N_STATIONS)]:\n",
    "        result[col] = result[col].astype(int)\n",
    "    #res_1.append(result)\n",
    "    return result\n",
    "\n",
    "@CudaTimer.timeit(name='event')\n",
    "def run_model(numpy_data, model):\n",
    "    with torch.no_grad():\n",
    "        inputs = setup(*numpy_data)\n",
    "        preds = model[0](*inputs)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "N_STATIONS = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_seeds_only_first_station(df, columns=['x','y','z']):\n",
    "    real = df\n",
    "    temp1 = real[real.station == 0]\n",
    "    st0_hits = temp1[columns].values\n",
    "    \n",
    "    seeds = torch.zeros((len(temp1), N_STATIONS, 3), dtype=torch.float32, device=DEVICE)\n",
    "    #seeds = np.zeros((len(temp1), N_STATIONS, 3))\n",
    "    seeds[:, 0, :] = torch.from_numpy(st0_hits)\n",
    "    return seeds\n",
    "\n",
    "def get_first_station_hits(df, columns=['x','y','z']):\n",
    "    real = df\n",
    "    temp1 = real[real.station == 0]\n",
    "    st0_hits = temp1[columns].values\n",
    "    return st0_hits\n",
    "\n",
    "def build_hits(target_df, cols = ['x', 'y', 'z']):\n",
    "    cont = torch.from_numpy(target_df[cols].values).contiguous().to(torch.float32).to(DEVICE)\n",
    "    return cont\n",
    "\n",
    "def parse_df(df, cols = ['x', 'y', 'z']):\n",
    "    first_hits = get_first_station_hits(df)\n",
    "    first_indexes = df[df.station == 0].index_old.values\n",
    "    \n",
    "    global_indexes = df.index_old.values\n",
    "    global_hits = df[cols].values.astype('float32')\n",
    "    \n",
    "    #hits_by_station = []\n",
    "    #indexes_by_station = []\n",
    "    #for station in range(N_STATIONS):\n",
    "    #    station_df = df.query('station == @station')\n",
    "    #    hits_by_station.append(station_df[['x', 'y', 'z']].values.astype('float32'))\n",
    "    #    indexes_by_station.append(station_df.index_old.values)\n",
    "    return first_hits, first_indexes, global_hits, global_indexes\n",
    "    #return first_hits, first_indexes, hits_by_station, indexes_by_station\n",
    "\n",
    "#@CudaTimer.timeit(name='setup')\n",
    "def setup(first_hits, first_indexes, global_hits, global_indexes):\n",
    "#def setup(first_hits, first_indexes, hits_by_station, indexes_by_station):\n",
    "    chunk_data_x = torch.zeros((len(first_hits), N_STATIONS, 4), dtype=torch.float32, device=DEVICE)\n",
    "    chunk_data_x[:, 0, :3] = torch.from_numpy(first_hits)\n",
    "    #r_axis = torch.sqrt(chunk_data_x[:, 0, 0]**2 + chunk_data_x[:, 0, 1]**2) + 0.02\n",
    "    chunk_data_x[:, :, 3] = 1.#r_axis\n",
    "    \n",
    "    chunk_data_len = torch.tensor(np.full(len(chunk_data_x), 1), dtype=torch.int64)\n",
    "    cand_mask = torch.ones(len(chunk_data_x), dtype=torch.int64, device=DEVICE)\n",
    "    hits_indexes_global = torch.full((len(chunk_data_x), 35), -1, dtype=torch.int64, device=DEVICE)\n",
    "    hits_indexes_global[:, 0] = torch.from_numpy(first_indexes)\n",
    "    \n",
    "    global_hits = torch.from_numpy(global_hits).to(DEVICE)\n",
    "    global_indexes = torch.from_numpy(global_indexes).to(DEVICE)\n",
    "    \n",
    "    #hits_by_station = [torch.from_numpy(hits).to(DEVICE) for hits in hits_by_station]\n",
    "    #indexes_by_station = [torch.from_numpy(indexes).to(DEVICE) for indexes in indexes_by_station]\n",
    "    return chunk_data_x, cand_mask, hits_indexes_global, global_indexes, global_hits\n",
    "    #return chunk_data_x, cand_mask, hits_indexes_global, hits_by_station, indexes_by_station\n",
    "\n",
    "#@torch.jit.script\n",
    "def drop_duplicated(cand_mask, chunk_data_x, hits_indexes_global):\n",
    "    duplicated = hits_indexes_global[:, 0] != hits_indexes_global[:, 1]\n",
    "    chunk_data_x = chunk_data_x[duplicated].contiguous()\n",
    "    hits_indexes_global = hits_indexes_global[duplicated]\n",
    "    cand_mask = cand_mask[duplicated]\n",
    "    return cand_mask, chunk_data_x, hits_indexes_global\n",
    "\n",
    "def build_df(indexes, event_df):\n",
    "    #preds, indexes = preds\n",
    "    new_columns = [*[f'hit_id_{i}' for i in range(N_STATIONS)], 'event', 'track_pred']\n",
    "    if len(indexes) == 0:\n",
    "        return pd.DataFrame(columns=new_columns).astype(float)\n",
    "    indexes = indexes.cpu().numpy()\n",
    "    #res_1.append(preds)\n",
    "    curr_event = event_df.event.values[0]\n",
    "    \n",
    "    \"\"\"\n",
    "    big_index = build_index(event_df)\n",
    "    event_idxs = np.array(event_df.index_old)\n",
    "    tracks_list = []\n",
    "    max_index = event_df.index.max()\n",
    "    \"\"\"\n",
    "    \n",
    "    #res_df = pd.DataFrame(columns=[f'hit_id_{i}' for i in range(N_STATIONS)])\n",
    "    #res_df = pd.DataFrame(data=np.full_like(indexes, -1), columns=[f'hit_id_{i}' for i in range(N_STATIONS)])\n",
    "    res_df = pd.DataFrame(data=indexes, columns=[f'hit_id_{i}' for i in range(N_STATIONS)])\n",
    "    res_df['event'] = curr_event\n",
    "    res_df['track_pred'] = True\n",
    "    res_df['track_pred'] = res_df['track_pred'].astype(bool)\n",
    "    #res_1.append(res_df.copy())\n",
    "    \n",
    "    #event_df.set_index(event_df.index_old, inplace=True)\n",
    "    #for idx, track_hits in enumerate(indexes):\n",
    "    #    track_hits = track_hits[track_hits > -1]\n",
    "    #    stations, indices = np.unique(event_df.loc[track_hits].station.values, return_index=True)\n",
    "    #    track_hits = track_hits[indices]\n",
    "    #    try:\n",
    "    #        res_df.loc[idx, [f'hit_id_{st}' for st in stations]] = track_hits\n",
    "    #    except:\n",
    "    #        res_df.drop(idx, inplace=True)\n",
    "    #res_df = res_df.fillna(-1)\n",
    "    \"\"\"\n",
    "    res_df = pd.DataFrame({'event': [curr_event] * len(preds),\n",
    "                           'track_pred': np.ones(len(preds))#.astype(bool),\n",
    "                          })\n",
    "    for i in range(N_STATIONS):\n",
    "        res_df[f'hit_id_{i}'] = indexes[:, i].numpy().astype(int)\n",
    "    #res_df.track_pred = res_df.track_pred.astype('bool')\n",
    "    #print(res_df)\"\"\"\n",
    "    #res_1.append(res_df.copy())\n",
    "    return res_df\n",
    "\n",
    "COLS = ['x', 'y', 'z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "class TrackNETInference(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        input_features = 4\n",
    "        hidden_features = 32\n",
    "        output_features = 3\n",
    "        batch_first = True\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size=input_features,\n",
    "            hidden_size=hidden_features,\n",
    "            num_layers=2,\n",
    "            batch_first=batch_first\n",
    "        )\n",
    "        \n",
    "        self.coords_1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_features, output_features)\n",
    "        )\n",
    "        self.radius_1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_features, 1),\n",
    "            torch.nn.Softplus()\n",
    "        )\n",
    "        self.coords_2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_features, output_features)\n",
    "        )\n",
    "        self.radius_2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_features, 1),\n",
    "            torch.nn.Softplus()\n",
    "        )\n",
    "        \n",
    "    #def forward(self, chunk_data_x, cand_mask, hits_indexes_global, hits_by_station: List[torch.Tensor], indexes_by_station: List[torch.Tensor]):\n",
    "    #@CudaTimer.timeit(name='event')\n",
    "    def forward(self, chunk_data_x, cand_mask, hits_indexes_global, global_indexes, global_hits):\n",
    "        # type: (Tensor, Tensor, Tensor, Tensor, Tensor) -> Tensor\n",
    "        #timer.start('event')\n",
    "        max_n_stations = 35\n",
    "\n",
    "        for stations_gone in range(1, max_n_stations):\n",
    "            #if stations_gone == 2:\n",
    "            #    ev_dropd_1.record(stream)\n",
    "            #timer.start('drop_duplicated')\n",
    "            cand_mask, chunk_data_x, hits_indexes_global = self.drop_duplicated(cand_mask, chunk_data_x, hits_indexes_global, stations_gone)\n",
    "            #timer.end('drop_duplicated')\n",
    "            #    ev_dropd_2.record(stream)\n",
    "            \n",
    "            if stations_gone == 4:\n",
    "                #timer.start('drop_short')\n",
    "                cand_mask, chunk_data_x, hits_indexes_global = self.drop_short(cand_mask, chunk_data_x, hits_indexes_global)\n",
    "                #timer.end('drop_short')\n",
    "            #if len(hits_by_station[stations_gone]) == 0:\n",
    "            #    break\n",
    "\n",
    "            #timer.start('exec_model')\n",
    "            prediction = self.exec_model(chunk_data_x, stations_gone)\n",
    "            #timer.end('exec_model')\n",
    "            #timer.start('search_torchdist')\n",
    "            distances, nearest_hits, hits_indexes = self.search_torchdist(prediction[:, :4], global_hits, global_indexes)\n",
    "            #timer.end('search_torchdist')\n",
    "            #distances, nearest_hits, hits_indexes = self.search_torchdist(single_prediction, hits_by_station[stations_gone], indexes_by_station[stations_gone])\n",
    "            #timer.start('filter')\n",
    "            in_ellipse_mask = self.filter_hits_by_distance(prediction, distances)\n",
    "            #timer.end('filter')\n",
    "            \n",
    "            not_ellipse_mask = ~in_ellipse_mask.to(torch.bool)\n",
    "            #timer.start('second_search')\n",
    "            distances2, nearest_hits2, hits_indexes2 = self.search_torchdist(prediction[not_ellipse_mask, 4:], global_hits, global_indexes)\n",
    "            #timer.end('second_search')\n",
    "            #timer.start('second_filter')\n",
    "            in_ellipse_mask2 = self.filter_hits_by_distance(prediction[not_ellipse_mask, 4:], distances2)\n",
    "            nearest_hits[not_ellipse_mask] = nearest_hits2\n",
    "            hits_indexes[not_ellipse_mask] = hits_indexes2\n",
    "            in_ellipse_mask[not_ellipse_mask] += in_ellipse_mask2\n",
    "            #timer.end('second_filter')\n",
    "            #print(in_ellipse_mask2)\n",
    "            #timer.start('prolong')\n",
    "            chunk_data_x, hits_indexes_global, cand_mask = self.prolong(in_ellipse_mask, chunk_data_x, stations_gone, nearest_hits, hits_indexes_global, hits_indexes, cand_mask)\n",
    "            #timer.end('prolong')\n",
    "        #timer.end('event')\n",
    "        last_mask = hits_indexes_global[:, -2] == hits_indexes_global[:, -1]\n",
    "        hits_indexes_global[last_mask, -1] = -1\n",
    "        #res_1.append(chunk_data_x)\n",
    "        return hits_indexes_global\n",
    "    \n",
    "    @CudaTimer.timeit(name='exec_model')\n",
    "    def exec_model(self, inputs, stations_gone: int):\n",
    "        x = inputs\n",
    "        x, _ = self.rnn(x)\n",
    "        coords_1 = self.coords_1(x)\n",
    "        radius_1 = self.radius_1(x)\n",
    "        coords_2 = self.coords_2(x)\n",
    "        radius_2 = self.radius_2(x)\n",
    "        outputs = torch.cat([coords_1, radius_1, coords_2, radius_2], dim=-1)\n",
    "        #outputs = torch.cat([coords_1, radius_1], dim=-1)\n",
    "        single_prediction = outputs[:, stations_gone - 1]\n",
    "        #if printing:\n",
    "        #    print(outputs)\n",
    "        return single_prediction\n",
    "    \n",
    "    @CudaTimer.timeit(name='drop_short')\n",
    "    def drop_short(self, cand_mask, chunk_data_x, hits_indexes_global):\n",
    "        mask = cand_mask.to(torch.bool)\n",
    "        chunk_data_x = chunk_data_x[mask].contiguous()\n",
    "        hits_indexes_global = hits_indexes_global[mask]\n",
    "        cand_mask = cand_mask[mask]\n",
    "        return cand_mask, chunk_data_x, hits_indexes_global\n",
    "    \n",
    "    @CudaTimer.timeit(name='drop_duplicated')\n",
    "    def drop_duplicated(self, cand_mask, chunk_data_x, hits_indexes_global, stations_gone: int):\n",
    "        duplicated = hits_indexes_global[:, stations_gone - 2] != hits_indexes_global[:, stations_gone - 1]\n",
    "        chunk_data_x = chunk_data_x[duplicated].contiguous()\n",
    "        hits_indexes_global = hits_indexes_global[duplicated]\n",
    "        cand_mask = cand_mask[duplicated]\n",
    "        return cand_mask, chunk_data_x, hits_indexes_global\n",
    "    \n",
    "    @CudaTimer.timeit(name='search_torchdist')\n",
    "    def search_torchdist(self, centers, global_hits, global_indexes):\n",
    "        centers = centers[:, :3]\n",
    "        D, I = torch.min(torch.cdist(centers, global_hits, compute_mode='use_mm_for_euclid_dist'), dim=1)\n",
    "        hits_indexes = global_indexes[I]\n",
    "        nearest_hits = global_hits[I]\n",
    "        return D, nearest_hits, hits_indexes\n",
    "    \n",
    "    @CudaTimer.timeit(name='filter')\n",
    "    def filter_hits_by_distance(self, ellipses, distances):\n",
    "        return (distances.flatten() < ellipses[:, 3]).int()\n",
    "    \n",
    "    @CudaTimer.timeit(name='prolong')\n",
    "    def prolong(self, in_ellipse_mask, chunk_data_x, stations_gone: int, nearest_hits, hits_indexes_global, hits_indexes, cand_mask):\n",
    "        cand_mask = in_ellipse_mask * cand_mask\n",
    "        chunk_data_x[:, stations_gone, :3] = nearest_hits * cand_mask.reshape(-1, 1)\n",
    "        r_axis = torch.sqrt(chunk_data_x[:, :, 0]**2 + chunk_data_x[:, :, 1]**2) + 0.02\n",
    "        chunk_data_x[:, :, 3] = r_axis\n",
    "        hits_indexes_global[:, stations_gone] = hits_indexes * cand_mask + cand_mask - 1\n",
    "        return chunk_data_x, hits_indexes_global, cand_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrackNETInference(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        input_features = 4\n",
    "        hidden_features = 32\n",
    "        output_features = 3\n",
    "        batch_first = True\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size=input_features,\n",
    "            hidden_size=hidden_features,\n",
    "            num_layers=2,\n",
    "            batch_first=batch_first\n",
    "        )\n",
    "        \n",
    "        self.coords_1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_features, output_features)\n",
    "        )\n",
    "        self.radius_1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_features, 1),\n",
    "            torch.nn.Softplus()\n",
    "        )\n",
    "        self.coords_2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_features, output_features)\n",
    "        )\n",
    "        self.radius_2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_features, 1),\n",
    "            torch.nn.Softplus()\n",
    "        )\n",
    "        \n",
    "    #@CudaTimer.timeit(name='event')\n",
    "    def forward(self, chunk_data_x, cand_mask, hits_indexes_global, global_indexes, global_hits):\n",
    "        # type: (Tensor, Tensor, Tensor, Tensor, Tensor) -> Tensor\n",
    "        max_n_stations = 35\n",
    "\n",
    "        for stations_gone in range(1, max_n_stations):\n",
    "            \n",
    "            if stations_gone == 4:\n",
    "                cand_mask, chunk_data_x, hits_indexes_global = self.drop_short(cand_mask, chunk_data_x, hits_indexes_global)\n",
    "            prediction = self.exec_model(chunk_data_x, stations_gone)\n",
    "            \n",
    "            first_dists, second_dists, first_nearest_hits, second_nearest_hits, first_hits_indexes, second_hits_indexes = self.search_both(prediction, global_hits, global_indexes)\n",
    "            \n",
    "            in_ellipse_mask1 = self.filter_hits_by_distance(prediction, first_dists)\n",
    "            in_ellipse_mask2 = self.filter_hits_by_distance(prediction[:, 4:], second_dists)\n",
    "            \n",
    "            #second_nearest_hits = prediction[:, :3]\n",
    "            #second_hits_indexes[:] = -1\n",
    "            \n",
    "            chunk_data_x, hits_indexes_global, cand_mask = self.prolong(in_ellipse_mask1, in_ellipse_mask2, chunk_data_x, stations_gone, first_nearest_hits, second_nearest_hits, hits_indexes_global, first_hits_indexes, second_hits_indexes, cand_mask)\n",
    "            \n",
    "            cand_mask, chunk_data_x, hits_indexes_global = self.drop_duplicated(cand_mask, chunk_data_x, hits_indexes_global, stations_gone)\n",
    "            \n",
    "        #last_mask = hits_indexes_global[:, -2] == hits_indexes_global[:, -1]\n",
    "        #hits_indexes_global[last_mask, -1] = -1\n",
    "        #res_1.append(chunk_data_x)\n",
    "        #res_1.append(hits_indexes_global)\n",
    "        return hits_indexes_global\n",
    "    \n",
    "    #@CudaTimer.timeit(name='exec_model')\n",
    "    def exec_model(self, inputs, stations_gone: int):\n",
    "        x = inputs\n",
    "        x, _ = self.rnn(x)\n",
    "        coords_1 = self.coords_1(x)\n",
    "        radius_1 = self.radius_1(x)\n",
    "        coords_2 = self.coords_2(x)\n",
    "        radius_2 = self.radius_2(x)\n",
    "        outputs = torch.cat([coords_1, radius_1, coords_2, radius_2], dim=-1)\n",
    "        #outputs = torch.cat([coords_1, radius_1], dim=-1)\n",
    "        single_prediction = outputs[:, stations_gone - 1]\n",
    "        #if printing:\n",
    "        #    print(outputs)\n",
    "        return single_prediction\n",
    "    \n",
    "    #@CudaTimer.timeit(name='drop_short')\n",
    "    def drop_short(self, cand_mask, chunk_data_x, hits_indexes_global):\n",
    "        mask = cand_mask.to(torch.bool)\n",
    "        chunk_data_x = chunk_data_x[mask].contiguous()\n",
    "        hits_indexes_global = hits_indexes_global[mask]\n",
    "        cand_mask = cand_mask[mask]\n",
    "        return cand_mask, chunk_data_x, hits_indexes_global\n",
    "    \n",
    "    #@CudaTimer.timeit(name='drop_duplicated')\n",
    "    def drop_duplicated(self, cand_mask, chunk_data_x, hits_indexes_global, stations_gone: int):\n",
    "        duplicated = torch.logical_or(hits_indexes_global[:, stations_gone - 2] != hits_indexes_global[:, stations_gone - 1], hits_indexes_global[:, stations_gone - 1] == -1)\n",
    "        chunk_data_x = chunk_data_x[duplicated].contiguous()\n",
    "        hits_indexes_global = hits_indexes_global[duplicated]\n",
    "        cand_mask = cand_mask[duplicated]\n",
    "        return cand_mask, chunk_data_x, hits_indexes_global\n",
    "    \n",
    "    #@CudaTimer.timeit(name='search_torchdist')\n",
    "    def search_torchdist(self, centers, global_hits, global_indexes):\n",
    "        centers = centers[:, :3]\n",
    "        D, I = torch.min(torch.cdist(centers, global_hits, compute_mode='use_mm_for_euclid_dist'), dim=1)\n",
    "        hits_indexes = global_indexes[I]\n",
    "        nearest_hits = global_hits[I]\n",
    "        return D, nearest_hits, hits_indexes\n",
    "    \n",
    "    #@CudaTimer.timeit(name='filter')\n",
    "    def filter_hits_by_distance(self, ellipses, distances):\n",
    "        return (distances.flatten() < ellipses[:, 3]).int()\n",
    "\n",
    "    #@CudaTimer.timeit(name='prolong')\n",
    "    def prolong(self, in_ellipse_mask1, in_ellipse_mask2, chunk_data_x, stations_gone: int, first_nearest_hits, second_nearest_hits, hits_indexes_global, first_hits_indexes, second_hits_indexes, cand_mask):\n",
    "        not_ellipse_mask = ~in_ellipse_mask1.to(torch.bool)\n",
    "        first_nearest_hits[not_ellipse_mask] = 0.#second_nearest_hits[not_ellipse_mask]\n",
    "        first_hits_indexes[not_ellipse_mask] = -1#second_hits_indexes[not_ellipse_mask]\n",
    "        in_ellipse_mask1[not_ellipse_mask] = in_ellipse_mask2[not_ellipse_mask]\n",
    "        \n",
    "        cand_mask = in_ellipse_mask1 * cand_mask\n",
    "        chunk_data_x[:, stations_gone, :3] = first_nearest_hits * cand_mask.reshape(-1, 1)\n",
    "        chunk_data_x[not_ellipse_mask, stations_gone] = 0.\n",
    "        #r_axis = torch.sqrt(chunk_data_x[:, :, 0]**2 + chunk_data_x[:, :, 1]**2) + 0.02\n",
    "        #chunk_data_x[:, :, 3] = r_axis\n",
    "        hits_indexes_global[:, stations_gone] = first_hits_indexes * cand_mask + cand_mask - 1\n",
    "        return chunk_data_x, hits_indexes_global, cand_mask\n",
    "    \n",
    "    #@CudaTimer.timeit(name='search_both')\n",
    "    def search_both(self, centers, global_hits, global_indexes):\n",
    "        flattened = centers.reshape((-1, 4))\n",
    "        flattened = flattened[:, :3]\n",
    "        D, I = torch.min(torch.cdist(flattened, global_hits, compute_mode='use_mm_for_euclid_dist'), dim=1)\n",
    "        dist = D.flatten().reshape(-1, 2)\n",
    "        indexes = I.reshape((-1, 2))\n",
    "        first_indexes = indexes[:, 0]\n",
    "        second_indexes = indexes[:, 1]\n",
    "        \n",
    "        first_hits_indexes = global_indexes[first_indexes]\n",
    "        first_nearest_hits = global_hits[first_indexes]\n",
    "        second_hits_indexes = global_indexes[second_indexes]\n",
    "        second_nearest_hits = global_hits[second_indexes]\n",
    "        first_dists = dist[:, 0]\n",
    "        second_dists = dist[:, 1]\n",
    "        return first_dists, second_dists, first_nearest_hits, second_nearest_hits, first_hits_indexes, second_hits_indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ariadne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ariadne_v2.transformations import AddVirtualPoints, DropEmptyFirstStation, ToCylindrical, DropFakes, DropOverPhi, CombineEvents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "STATION_COLUMNS = [f'hit_id_{i}' for i in range(N_STATIONS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['event', 'x', 'y', 'z', 'station', 'track', 'px', 'py', 'pz', 'vtxx', 'vtxy', 'vtxz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "parse_cfg = {\n",
    "    'csv_params' : {\n",
    "        \"sep\": '\\t',\n",
    "        \"encoding\": 'utf-8',\n",
    "        \"names\": columns\n",
    "    },\n",
    "    'input_file_mask':f'/lustre/home/user/d/drusov/ariadne_spd/ariadne/output/spd_tracknet_train_raw/spdsim_test_missinghits01_fakesnsqr_200k.tsv',\n",
    "    'events_quantity': ':'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluator = EventEvaluator(parse_cfg, global_transformer, N_STATIONS)\n",
    "events = evaluator.prepare(model_loader=loaded_model)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loaded_model = TrackNetModelLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "constraints = {'x': [-851., 851.], 'y': [-851., 851.], 'z': [-2386., 2386.]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "global_transformer = Compose([\n",
    "    DropShort(num_stations=3),\n",
    "    DropEmptyFirstStation(),\n",
    "    #DropTracksWithHoles(),\n",
    "    #DropSpinningTracks(),\n",
    "    #ToCylindrical(),\n",
    "    #BakeStationValues(r_values, col='r'),\n",
    "    ConstraintsNormalize(constraints=constraints, columns=('x', 'y', 'z')),\n",
    "    #DropOverPhi(),\n",
    "    #DropFakes(),\n",
    "    CombineEvents(combine_n=40),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gin.enter_interactive_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "printing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1 = []\n",
    "res_2 = []\n",
    "times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read entry 62215056bdd09505e7cbf81302bb5f21 hit\n",
      "[prepare]: started processing a df spdsim_test_missinghits01_fakesnsqr_200k.tsv with 199733753 rows:\n",
      "read entry 087222e21a242674ddebc928376a08ce hit\n",
      "[prepare] finished\n",
      "[prepare] loading your model(s)...\n",
      "/lustre/home/user/d/drusov/ariadne_spd/ariadne/lightning_logs/TrackNETMissingHits/version_19/*.ckpt\n",
      "[prepare] finished loading your model(s)...\n",
      "[build_all_tracks] start\n",
      "read entry d54bbf42e409b5e3805b0fb357d40aac hit\n",
      "read entry d12d1296d5dd9b010b06131efb0bec74 hit\n",
      "[build_all_tracks] cache hit, finish\n",
      "2.166612148284912\n",
      "9.613042116165161\n",
      "0.06113481521606445\n",
      "0.061582326889038086\n",
      "0.060616493225097656\n",
      "[run model] start\n",
      "\n",
      "\n",
      "preprocessing: processed: 4999: 100%|██████████| 5000/5000 [00:40<00:00, 123.18it/s]\n",
      "model run: processed: 4999: 100%|██████████| 5000/5000 [05:49<00:00, 14.29it/s]\n",
      "[run model] cache miss, finish\n",
      "[solve results] start\n",
      "[solve results] finish\n",
      "[solve results] final stats:\n",
      "==========EVALUATION RESULTS==========\n",
      "Total events evaluated: 5000\n",
      "Total tracks evaluated: 982026\n",
      "Track Efficiency (recall): 0.9333 \n",
      "Track Purity (precision): 0.8306 \n",
      "Fully reconstructed event ratio: 0.0000\n",
      "Mean cpu time per event: 0.0039 sec (257.53 events per second) \n",
      "Mean gpu time per event: 0.0626 sec (15.98 events per second) \n",
      "==========EVALUATION RESULTS==========\n",
      "solving took 19.427649974822998 seconds\n"
     ]
    }
   ],
   "source": [
    "evaluator = EventEvaluator(parse_cfg, global_transformer, N_STATIONS)\n",
    "events = evaluator.prepare(model_loader=loaded_model)[0]\n",
    "all_results = evaluator.build_all_tracks()\n",
    "\n",
    "# COLD START\n",
    "\n",
    "model = evaluator.loaded_model_state[1][0]\n",
    "numpy_data = np.load('cold_start.npy', allow_pickle=True)\n",
    "inputs = setup(*numpy_data)\n",
    "for i in range(5):\n",
    "    start_time = time.time()\n",
    "    preds = model(*inputs)\n",
    "    torch.cuda.synchronize()\n",
    "    print(time.time() - start_time)\n",
    "#model.save(\"torchscript_model.pt\")\n",
    "CudaTimer.clear()\n",
    "model_results = evaluator.run_model(preprocess_one_event, run_tracknet_eval)\n",
    "start_time = time.time()\n",
    "results_tracknet = evaluator.solve_results(model_results, all_results, match_percentage=1)\n",
    "end_time = time.time()\n",
    "print(f'solving took {end_time - start_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = CudaTimer.times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_np = np.array(times['event'])\n",
    "print(f'{1000 / times_np.mean()}, *40 = {40000 / times_np.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "times['event'] = np.array(times['event'])\n",
    "times['setup'] = np.array(times['setup'])\n",
    "times['drop_short'] = np.array(times['drop_short'])\n",
    "times['exec_model'] = np.array(times['exec_model'])\n",
    "times['filter'] = np.array(times['filter'])\n",
    "times['search_both'] = np.array(times['search_both'])\n",
    "times['prolong'] = np.array(times['prolong'])\n",
    "times['drop_duplicated'] = np.array(times['drop_duplicated'])\n",
    "#times['second_search'] = np.array(times['second_search'])\n",
    "#times['second_filter'] = np.array(times['second_filter'])\n",
    "\n",
    "for key in times.keys():\n",
    "    print(f'{key} time: {times[key].sum():.2f} ms, mean {times[key].mean():.2f} ms, ratio {times[key].sum() / times[\"event\"].sum():.2f}')\n",
    "print()\n",
    "#print(f'Total time: {times[\"event\"].sum():.2f} ms, mean {times[\"event\"].mean():.2f} ms, timeslices per sec {40000 / times[\"event\"].mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1 = res_1[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.query('event == 6 and track == 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1[12][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1[13][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = evaluator.loaded_model_state[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.exec_model(hits_13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tracknet[0].query('pred == 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = results_tracknet[0].copy()\n",
    "tracks['len'] = N_STATIONS - np.argmax(np.fliplr(tracks[STATION_COLUMNS] >= 0), axis=1)\n",
    "tracks['with_holes'] = tracks.len - (tracks[STATION_COLUMNS] >= 0).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks['first_hole'] = 0\n",
    "for st in range(34, -1, -1):\n",
    "    st_col = f'hit_id_{st}'\n",
    "    tracks['first_hole'][tracks[st_col] == -1] = st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks.query('pred == 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tracks.with_holes == 0).sum() / len(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N_STATIONS - 1):\n",
    "    if (((tracks[STATION_COLUMNS[i]] == -1) + (tracks[STATION_COLUMNS[i + 1]] == -1)) == 2).any():\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(tracks.with_holes, return_counts=True)[0], np.unique(tracks.with_holes, return_counts=True)[1] / len(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks.query('pred == 0 and first_hole == 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks.query('event_id == 19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks.query('pred != -1 and with_holes > 0').first_hole.hist(bins=33)\n",
    "plt.title(\"First hole on ? station (all tracks)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks.query('pred == 0 and with_holes > 0').first_hole.hist(bins=33)\n",
    "plt.title(\"First hole on ? station (not found tracks)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(625)):\n",
    "    os.mkdir(f'faiss_samples/event{i}')\n",
    "    with open(f'faiss_samples/event{i}/index.npy', 'wb') as f:\n",
    "        np.save(f, res_1[i])\n",
    "    os.mkdir(f'faiss_samples/event{i}/samples')\n",
    "    for j in range(34):\n",
    "        with open(f'faiss_samples/event{i}/samples/query{j}.npy', 'wb') as f:\n",
    "            np.save(f, res_2[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indexes = []\n",
    "queries = []\n",
    "for i in tqdm(range(625)):\n",
    "    index_np = np.load(f'faiss_samples/event{i}/index.npy')\n",
    "    indexes.append(torch.from_numpy(index_np))\n",
    "    queries.append([])\n",
    "    for j in range(34):\n",
    "        query = np.load(f'faiss_samples/event{i}/samples/query{j}.npy')\n",
    "        queries[-1].append(torch.from_numpy(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_f = []\n",
    "for index_np in tqdm(indexes):\n",
    "    index = faiss.IndexFlatL2(3)\n",
    "    if DEVICE == 'cuda':\n",
    "        index = faiss.index_cpu_to_gpu(faiss_gpu, 0, index)\n",
    "    index.add(index_np)\n",
    "    indexes_f.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "times = []\n",
    "event_times = []\n",
    "for index, queries_curr in tqdm(zip(indexes_f, queries)):\n",
    "    event_start = time.time()\n",
    "    #index = index.to(DEVICE)\n",
    "    queries_gpu = [query.to(DEVICE) for query in queries_curr]\n",
    "    for query in queries_gpu:\n",
    "        #start_time = time.time()\n",
    "        pr.enable()\n",
    "        dist, i = index.search(query, 1)\n",
    "        pr.disable()\n",
    "        #times.append(time.time() - start_time)\n",
    "    event_times.append(time.time() - event_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(times).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(event_times).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "event_times = []\n",
    "for index, queries_curr in tqdm(zip(indexes, queries)):\n",
    "    event_start = time.time()\n",
    "    index = index.to(DEVICE)\n",
    "    queries_gpu = [query.to(DEVICE) for query in queries_curr]\n",
    "    for query in queries_gpu:\n",
    "        #start_time = time.time()\n",
    "        pr.enable()\n",
    "        dist, i = search_t(query, index)\n",
    "        pr.disable()\n",
    "        #times.append(time.time() - start_time)\n",
    "    event_times.append(time.time() - event_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(times).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(event_times).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes[0].shape, queries[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = indexes_f[0].search(queries[0][0], 1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.min(torch.cdist(queries[0][0], indexes[0]), dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a[1].flatten() == b[1]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a[0].flatten() == b[0]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(b[0]**2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = 0\n",
    "for ev_id in tqdm(range(625)):\n",
    "    index = faiss.IndexFlatL2(3)\n",
    "    index.add(res_1[ev_id])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i in range(34):\n",
    "        dist, i = index.search(res_2[ev_id][i], 1)\n",
    "    total_time += time.time() - start_time\n",
    "total_time / 625    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = profile.Profile()\n",
    "pr.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr.enable()\n",
    "for pair in tqdm(res_1, smoothing=0):\n",
    "    a = search_distance(pair[0].cpu(), pair[1], 1)\n",
    "pr.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in tqdm(res_1, smoothing=0):\n",
    "    #pr.enable()\n",
    "    a = pair[1].range_search(pair[0].cpu().contiguous(), 0.00000087)\n",
    "    #pr.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr.dump_stats('profile1.pstat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = results_tracknet[0].copy()\n",
    "for ev in sorted(result.event_id.unique()):\n",
    "    n_ghosts = len(result.query('event_id == @ev and pred == -1'))\n",
    "    if n_ghosts > 0:\n",
    "        max_track = result.query('event_id == @ev').track.max()\n",
    "        result.track[(result.event_id == ev) & (result.pred ==-1)] = list(range(max_track + 1, max_track + n_ghosts + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_1 = events.set_index('index_old')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = []\n",
    "for i in tqdm(range(0, 25000, 100)):\n",
    "    result_part = result.query('@i <= event_id < @i + 100')\n",
    "    events_1_part = events_1.query('@i <= event < @i + 100')\n",
    "    for track in result_part.itertuples():\n",
    "        hits_ids = np.array(track[7:])\n",
    "        hits_df = events_1_part.loc[hits_ids[hits_ids > 0]]\n",
    "        hits_df.track = track.track\n",
    "        hits_df['pred'] = track.pred\n",
    "        hits.append(hits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm_coord(coord, min_val, max_val):\n",
    "   return ((coord + 1) * (max_val - min_val) / 2) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_to_kf = pd.concat(hits)\n",
    "hits_to_kf.x = denorm_coord(hits_to_kf.x, constraints['x'][0], constraints['x'][1])\n",
    "hits_to_kf.y = denorm_coord(hits_to_kf.y, constraints['y'][0], constraints['y'][1])\n",
    "hits_to_kf.z = denorm_coord(hits_to_kf.z, constraints['z'][0], constraints['z'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_to_kf.sort_values('event', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hits_to_kf.to_csv('1.txt', columns=['event', 'x', 'y', 'z', 'station', 'track', 'pred', 'px', 'py', 'pz', 'vtxx', 'vtxy', 'vtxz'], sep=\"\\t\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_to_kf.query('1998 <= event < 2001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pxpypz = pd.read_csv('result.csv', names=['event_id', 'track', 'px', 'py', 'pz'])\n",
    "pxpypz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(pxpypz, result, how='inner',\n",
    "                           on=['event_id', 'track'],\n",
    "                           suffixes=[None, '_y'])\n",
    "merged_df.drop(['px_y', 'py_y', 'pz_y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merged_df['track'][merged_df.pred == -1] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merged_df.pred = merged_df.pred.astype('int32')\n",
    "merged_df.track = merged_df.track.astype('int32')\n",
    "merged_df.event_id = merged_df.event_id.astype('int32')\n",
    "\n",
    "merged_df.px = merged_df.px.astype('float32')\n",
    "merged_df.py = merged_df.py.astype('float32')\n",
    "merged_df.pz = merged_df.pz.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_tracks = []\n",
    "ind_col = []\n",
    "for i in tqdm(range(0, 25000, 100)):\n",
    "    result_part = result.query('@i <= event_id < @i + 100')\n",
    "    pxpypz_part = pxpypz.query('@i <= event_id < @i + 100')\n",
    "    for track in result_part.itertuples():\n",
    "        if track.pred == -1:\n",
    "            pxpypz_part_track = pxpypz_part.query('event_id == @track.event_id and track == @track.track')\n",
    "            if len(pxpypz_part_track) > 0:\n",
    "                track_dict = {}\n",
    "                ind_col.append(track.Index)\n",
    "                #track['index'] = pxpypz_part_track.index[0]\n",
    "                track_dict['px'] = pxpypz_part_track.px.iloc[0]\n",
    "                track_dict['py'] = pxpypz_part_track.py.iloc[0]\n",
    "                track_dict['pz'] = pxpypz_part_track.pz.iloc[0]\n",
    "                new_tracks.append(track_dict)\n",
    "new_tracks = pd.DataFrame(new_tracks)\n",
    "new_tracks = new_tracks.astype(result[['px', 'py', 'pz']].dtypes)\n",
    "new_tracks.set_index(np.array(ind_col), inplace=True)\n",
    "new_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_res = result.join(new_tracks, rsuffix='_kf')\n",
    "joined_res.px = joined_res.px.fillna(joined_res.px_kf)\n",
    "joined_res.py = joined_res.py.fillna(joined_res.py_kf)\n",
    "joined_res.pz = joined_res.pz.fillna(joined_res.pz_kf)\n",
    "joined_res.drop(['px_kf', 'py_kf', 'pz_kf'], axis=1, inplace=True)\n",
    "joined_res.track[joined_res.pred == -1] = -1\n",
    "joined_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tracknet[0].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "joined_res.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "results_tracknet = evaluator.solve_results(model_results, all_results, match_percentage=1)\n",
    "end_time = time.time()\n",
    "print(f'solving took {end_time - start_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tracknet[0].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def draw_track(ax, x, y, z, label, track_num):\n",
    "    if label == 1:\n",
    "        col = 'green'#(0., 1.0, 0.000, 0.9)\n",
    "    elif label == 0:\n",
    "        col = (0, 0.0, 1, 0.9)\n",
    "    elif label == -1:\n",
    "        pass\n",
    "        col = (1, 0.0, 0.000, 0.9)\n",
    "    else:\n",
    "        col = (0, 1, 0, 0.3)\n",
    "    val_x = x#track[:,0]\n",
    "    val_y = y#track[:,2]\n",
    "    val_z = z#track[:,1# ]\n",
    "    #if track_num != 22 and track_num != 1285:\n",
    "    #    col = list(col)[:3] + [0]\n",
    "    line, = ax.plot(val_x, val_y, zs=val_z, color=col, label=f'track_num={track_num}', picker=10)\n",
    "    #ax.scatter(val_x, val_y, zs=val_z, color=col)\n",
    "    return line\n",
    "\n",
    "\n",
    "def draw_event(ev_id, events_df, projection='xz', draw_unproc=False, columns=['x', 'y', 'z']):\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    ax = plt.subplot(111, projection=\"3d\")\n",
    "    fig.add_axes(ax)\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    if projection == 'xz':\n",
    "        ax.view_init(azim=200, elev=10)\n",
    "        ax.set_xlabel(columns[0])\n",
    "    else:\n",
    "        ax.view_init(azim=360, elev=0)\n",
    "        ax.set_ylabel(columns[1])\n",
    "    i = 0\n",
    "    lines = []\n",
    "\n",
    "    hits_real = events_df.query('event == @ev_id and track >= 0')\n",
    "    ax.scatter(hits_real[columns[0]], hits_real[columns[1]], zs=hits_real[columns[2]], c='lightgreen', alpha=0.5)\n",
    "    \n",
    "    hits_fake = events_df.query('event == @ev_id and track == -1')[:100]\n",
    "    ax.scatter(hits_fake[columns[0]], hits_fake[columns[1]], zs=hits_fake[columns[2]], c='red', alpha=0.5)\n",
    "\n",
    "    for tr_id, df in events_df.query('event == @ev_id').groupby('track'):\n",
    "        if tr_id != -1:\n",
    "            lines.append(draw_track(ax, df[columns[0]], df[columns[1]], df[columns[2]], 1, tr_id))\n",
    "\n",
    "    #leg = ax.legend(fancybox=True, shadow=True)\n",
    "    #lined = {}  # Will map legend lines to original lines.\n",
    "    #for legline, origline in zip(leg.get_lines(), lines):\n",
    "    #    legline.set_picker(True)  # Enable picking on the legend line.\n",
    "    #    lined[legline] = origline\n",
    "\n",
    "    \"\"\"def on_pick(event):\n",
    "        # On the pick event, find the original line corresponding to the legend\n",
    "        # proxy line, and toggle its visibility.\n",
    "        legline = event.artist\n",
    "        origline = lined[legline]\n",
    "        visible = not origline.get_visible()\n",
    "        origline.set_visible(visible)\n",
    "        # Change the alpha on the line in the legend so we can see what lines\n",
    "        # have been toggled.\n",
    "        legline.set_alpha(1.0 if visible else 0.2)\n",
    "        fig.canvas.draw()\n",
    "\n",
    "    fig.canvas.mpl_connect('pick_event', on_pick)\"\"\"\n",
    "    plt.savefig('spd_event.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_event(7, events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = results_tracknet[0].copy()\n",
    "result['len'] = (result[STATION_COLUMNS] < 0).values.argmax(axis=1)\n",
    "result.len = result.len.replace(0, N_STATIONS)\n",
    "\n",
    "for tr_len in sorted(list(result.len.unique())):\n",
    "    cands = result.query('len == @tr_len')\n",
    "    try:\n",
    "        precision = len(cands.query('pred == 1')) / (len(cands.query('pred == -1')) + len(cands.query('pred == 1')))\n",
    "    except:\n",
    "        precision = 0\n",
    "    try:\n",
    "        recall = len(cands.query('pred == 1')) / (len(cands.query('pred == 0')) + len(cands.query('pred == 1')))\n",
    "    except:\n",
    "        recall = 0\n",
    "    print(f'tracklen = {tr_len}, number = {len(cands.query(\"pred != -1\"))}({len(cands.query(\"pred != -1\"))/len(result.query(\"pred != -1\"))}), precision {precision}, recall {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "from scipy import stats\n",
    "\n",
    "MODES = {\n",
    "    \"RECALL\":1,\n",
    "    \"PRECISION\":2\n",
    "}\n",
    "a = []\n",
    "def get_diagram_arr_linspace(all_real_hits, found_hits, start, end, num, col, mode):\n",
    "    spac = np.linspace(start, end, num=num)\n",
    "    #print(num)\n",
    "    step = (spac[1] - spac[0]) / 2\n",
    "    arr = []\n",
    "    spac_ret = []\n",
    "    track_count_for_arr = []\n",
    "    for i in range(len(spac)-1):\n",
    "        beg = spac[i]\n",
    "        end = spac[i+1]\n",
    "        elems_real = all_real_hits[(all_real_hits[col] >= beg) & (all_real_hits[col] < end)]\n",
    "        elems_pred = found_hits[(found_hits[col] >= beg) & (found_hits[col] < end)]\n",
    "        #print(beg,end)\n",
    "        #print(len(elems_pred), len(elems_real))\n",
    "        if mode == MODES[\"RECALL\"]:\n",
    "            if elems_real.empty:\n",
    "                #arr.append(1.)\n",
    "                continue\n",
    "            else:\n",
    "                arr.append(len(elems_pred) / len(elems_real))\n",
    "                track_count_for_arr.append(len(elems_real))\n",
    "        elif mode == MODES[\"PRECISION\"]:\n",
    "            if elems_pred.empty:\n",
    "                #arr.append(1.)\n",
    "                continue\n",
    "            else:\n",
    "                reco_true = len(elems_pred[elems_pred.track != -1])\n",
    "                arr.append(reco_true / len(elems_pred))\n",
    "                track_count_for_arr.append(len(elems_real))\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        spac_ret.append(spac[i])\n",
    "\n",
    "    return np.array(arr), np.array(spac_ret), np.array(track_count_for_arr)\n",
    "\n",
    "\n",
    "def get_diagram_for_boxplot(all_real_hits, found_hits, start, end, num, col, bin_array, int_mode, mode):\n",
    "    y_vals, x_vals, counts_ = get_diagram_arr_linspace(all_real_hits, found_hits, start, end, num, col, mode)\n",
    "\n",
    "    np_y_vals = np.array(y_vals)\n",
    "    bin_array = bin_array\n",
    "\n",
    "\n",
    "    res_x_array = []\n",
    "    res_box_data = []\n",
    "    mean_box_array = []\n",
    "    track_counts_ed = []\n",
    "    for i in range(len(bin_array)-1):\n",
    "        beg = bin_array[i]\n",
    "        end = bin_array[i+1]\n",
    "        y_ind = np.where((x_vals>=beg) & (x_vals<end))\n",
    "        y_vals_corr  = np_y_vals[y_ind]\n",
    "        track_counts_corr = counts_[y_ind]\n",
    "\n",
    "        y_vals_corr = y_vals_corr[~np.isnan(y_vals_corr)]\n",
    "        if len(y_vals_corr) == 0:\n",
    "            continue\n",
    "\n",
    "        #print(beg, end, i, stats.describe(y_vals_corr))\n",
    "        #print(y_vals_corr)\n",
    "        #         if len(np.where(y_vals_corr < 1)) > 0:\n",
    "        #             print(y_vals_corr)\n",
    "        #             print(y_vals)\n",
    "        #             print(beg,end)\n",
    "\n",
    "        res_box_data.append(y_vals_corr)\n",
    "        track_counts_ed.append(np.sum(track_counts_corr))\n",
    "\n",
    "        delta = 0 if int_mode else (end-beg)/2\n",
    "        res_x_array.append(beg + delta)\n",
    "        mean_box_array.append(np.mean(y_vals_corr))\n",
    "\n",
    "    return res_box_data, np.array(res_x_array), np.array(mean_box_array), np.array(track_counts_ed)\n",
    "    #plt.boxplot(res_box_data, positions=bin_array)\n",
    "\n",
    "def boxplot_style(bp):\n",
    "    #for box in bp['boxes']:\n",
    "    # change outline color\n",
    "    #box.set( color='#7570b3', linewidth=2)\n",
    "    # change fill color\n",
    "    #box.set( facecolor = 'silver' )\n",
    "\n",
    "    ## change color and linewidth of the whiskers\n",
    "    #for whisker in bp['whiskers']:\n",
    "    #    whisker.set(color='#7570b3', linewidth=2)\n",
    "    #\n",
    "    ### change color and linewidth of the caps\n",
    "    #for cap in bp['caps']:\n",
    "    #    cap.set(color='#7570b3', linewidth=2)\n",
    "    #\n",
    "    ### change color and linewidth of the medians\n",
    "    #for median in bp['medians']:\n",
    "    #    median.set(color='tab:red', linewidth=3, zorder=30)\n",
    "\n",
    "    for median in bp['means']:\n",
    "        median.set(color='tab:green', linewidth=2, ls='-', zorder=10)\n",
    "\n",
    "\n",
    "##########################\n",
    "###########################\n",
    "\n",
    "from numpy import linalg as LA\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "\n",
    "def draw_for_col(ax, tracks_real, tracks_pred_true,\n",
    "                 col, col_pretty, total_events, n_ticks=150,\n",
    "                 n_avg_ticks=-1, x_ticks=8,\n",
    "                 custom_title=None,\n",
    "                 ticks_custom=True,\n",
    "                 with_boxplot=False,\n",
    "                 int_mode=False, save_disk=True,\n",
    "                 custom_draw_funcs=[],\n",
    "                 diagram_func=None,\n",
    "                 color_ax_ticks=False,\n",
    "                 subtitle=None,\n",
    "                 model_name=\"NO_MODEL_NAME\",\n",
    "                 graph_idx=0, total_indices=-1, spacing=-1,\n",
    "                 color_line='tab:orange', color_box='red', mode=-1\n",
    "                 ):\n",
    "    real_xticks_count = x_ticks\n",
    "\n",
    "    color_ax_ticks = len(custom_draw_funcs) > 0 and color_ax_ticks\n",
    "    n_avg_ticks = n_ticks // 5 if n_avg_ticks < 0 else n_avg_ticks\n",
    "\n",
    "    delta = 1e-4 if not int_mode else 1\n",
    "\n",
    "    start = tracks_real[tracks_real[col] > -np.inf][col].min()\n",
    "    end = tracks_real[tracks_real[col] < np.inf][col].max()+delta\n",
    "\n",
    "    initial, spac, counts_ = get_diagram_arr_linspace(tracks_real, tracks_pred_true, start, end, n_ticks, col, mode)\n",
    "\n",
    "    maxX = int(end) if int_mode else end\n",
    "    ax.set_xlabel(col_pretty)\n",
    "    ax.plot(spac, initial, alpha=0.0, lw=1)\n",
    "\n",
    "    if not int_mode and ticks_custom:\n",
    "        ax.set_xticks(np.linspace(start, maxX, x_ticks))\n",
    "    else:\n",
    "        ax.locator_params(axis='x', nbins=x_ticks)\n",
    "\n",
    "    if diagram_func is None:\n",
    "        ax.set_yticks(np.round(np.linspace(0, 1, 11), decimals=2))\n",
    "    x_ticks = ax.get_xticks()\n",
    "\n",
    "    if with_boxplot:\n",
    "        old_ticks = x_ticks\n",
    "        delta_x = (x_ticks[1] - x_ticks[0])/2\n",
    "\n",
    "        diagram_func = get_diagram_for_boxplot if diagram_func is None else diagram_func\n",
    "\n",
    "        box_data, ticks_x, mean_data, counts_ed = diagram_func(tracks_real, tracks_pred_true, start, end,\n",
    "                                                               n_ticks, col, x_ticks, int_mode, mode)\n",
    "        #print(x_ticks)\n",
    "        if total_indices > 0:\n",
    "\n",
    "            width_tuned = delta_x * 2 / total_indices\n",
    "            pos_tuned = ticks_x - delta_x\n",
    "            spacings = width_tuned * 0.4\n",
    "            width_real = width_tuned * 0.6\n",
    "\n",
    "            ticks_x = pos_tuned + width_tuned * graph_idx + spacings/2 + width_real/2\n",
    "            delta_x = width_real\n",
    "        box_data_t = []\n",
    "        ticks_x_t = []\n",
    "        delta_x_t = []\n",
    "        mean_data_t = []\n",
    "        counts_ed_t = []\n",
    "\n",
    "        for i, box in enumerate(box_data):\n",
    "            if len(box) > 4:\n",
    "                box_data_t.append(box)\n",
    "                ticks_x_t.append(ticks_x[i])\n",
    "                mean_data_t.append(mean_data[i])\n",
    "\n",
    "                counts_ed_t.append(counts_ed[i])\n",
    "\n",
    "        box_data = np.array(box_data_t)\n",
    "        ticks_x = np.array(ticks_x_t)\n",
    "        mean_data = np.array(mean_data_t)\n",
    "\n",
    "        counts_ed = np.array(counts_ed_t)\n",
    "\n",
    "        bp = ax.boxplot(box_data, positions=ticks_x,\n",
    "                        manage_ticks=False, meanline=True, showmeans=True,\n",
    "                        widths=delta_x,patch_artist=True, sym='',zorder=3,boxprops=dict(facecolor=color_box, alpha=0.3))\n",
    "        boxplot_style(bp)\n",
    "        ret_widths = delta_x\n",
    "        # mean line\n",
    "        xnew = np.linspace(ticks_x.min(), ticks_x.max(), 500)\n",
    "        spl = make_interp_spline(ticks_x, mean_data, k=1)\n",
    "        power_smooth = spl(xnew)\n",
    "        ax.plot(xnew, power_smooth, ls='--', color=color_line, label=f'{model_name} mean track {subtitle}', lw=3, zorder=15)\n",
    "        #ax.set_xticks(old_ticks)\n",
    "        if graph_idx == 0:\n",
    "            for i in range(len(ticks_x)):\n",
    "                y_pos = 1.01\n",
    "                x_pos = ticks_x[i] - delta_x/2\n",
    "                ax.text(x_pos, y_pos, f\"{counts_ed[i]}\", zorder=80)\n",
    "\n",
    "    if int_mode or not ticks_custom:\n",
    "        ax.locator_params(axis='x', nbins=real_xticks_count)\n",
    "\n",
    "    #if diagram_func is None:\n",
    "    #ax.set_ylim((0.8, 1.05))\n",
    "    ax.set_ylim((-0.05, 1.05))\n",
    "    ticks = ax.get_xticks()\n",
    "    step = ticks[1] - ticks[0]\n",
    "    if graph_idx == total_indices-1:\n",
    "        ax.set_xlim((np.min(ticks_x) - width_tuned-step, np.max(ticks_x)+width_real + step/2))\n",
    "\n",
    "    for draw_f in custom_draw_funcs:\n",
    "        draw_f(ax)\n",
    "\n",
    "    return ret_widths\n",
    "\n",
    "    ####plt.locator_params(axis='y', nbins=16)\n",
    "\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    #plt.rcParams['savefig.facecolor']='white'\n",
    "    #os.makedirs('../output', exist_ok=True)\n",
    "    #plt.savefig('../output/new_img_track_eff_%s_ev%r_t%d.png'%(col, total_events, n_ticks), dpi=300)\n",
    "    #plt.show()\n",
    "\n",
    "def draw_for_raw_data(ax, data_x, data_y, data_y_err, color_box, color_line, col_widths, model_name, subtitle):\n",
    "    data_x = np.array(data_x)\n",
    "    data_y_init = np.array(data_y)\n",
    "    dataep = data_y + np.array(data_y_err)\n",
    "    dataem = data_y - np.array(data_y_err)\n",
    "\n",
    "    data_y = np.expand_dims(data_y,axis=-1)\n",
    "    dataep = np.expand_dims(dataep,axis=-1)\n",
    "    dataem = np.expand_dims(dataem,axis=-1)\n",
    "\n",
    "    data_y = np.concatenate((data_y, dataep, dataem), axis=1).T\n",
    "    delta_x = (data_x[1]-data_x[0]) / 2\n",
    "\n",
    "    width_tuned = delta_x * 2 / 3\n",
    "    pos_tuned = data_x - delta_x\n",
    "    spacings = width_tuned * 0.4\n",
    "    width_real = width_tuned * 0.6\n",
    "\n",
    "    ticks_x = pos_tuned + width_tuned * 2 + spacings/2 + width_real/2\n",
    "\n",
    "    bp = ax.boxplot(data_y, positions=ticks_x,\n",
    "                    manage_ticks=False, meanline=True, showmeans=True,\n",
    "                    widths=col_widths, patch_artist=True, sym='',zorder=3,boxprops=dict(facecolor=color_box, alpha=0.3))\n",
    "    boxplot_style(bp)\n",
    "    xnew = np.linspace(ticks_x.min(), ticks_x.max(), 500)\n",
    "    mean_data = data_y_init\n",
    "    spl = make_interp_spline(ticks_x, mean_data, k=1)  # type: BSpline\n",
    "    power_smooth = spl(xnew)\n",
    "\n",
    "    ax.plot(xnew, power_smooth, ls='--', color=color_line, label=f'{model_name} mean track {subtitle}', lw=3, zorder=15)\n",
    "\n",
    "    pass\n",
    "\n",
    "def to_recall(all_tracks_df):\n",
    "    tracks_real = all_tracks_df[all_tracks_df.pred != -1]\n",
    "    tracks_pred_true = all_tracks_df[all_tracks_df.pred == 1]\n",
    "    return tracks_real, tracks_pred_true, MODES[\"RECALL\"], \"efficiency\"\n",
    "\n",
    "def to_precision(all_tracks_df):\n",
    "    tracks_real = all_tracks_df[all_tracks_df.track != -1]\n",
    "    tracks_pred_true = all_tracks_df[(all_tracks_df.pred == 1) | (all_tracks_df.pred == -1)]\n",
    "    return tracks_real, tracks_pred_true, MODES[\"PRECISION\"], \"purity\"\n",
    "\n",
    "def plot_model_results(model_names, model_results_arr, mode_func, funcs_for_kalman=None):\n",
    "\n",
    "    fig = plt.figure(figsize=(14,16), dpi=80)\n",
    "    ax1, ax2, ax3 = fig.subplots(nrows=3, ncols=1)\n",
    "    #    ax1, ax2 = fig.subplots(nrows=1, ncols=2, sharey='all')\n",
    "\n",
    "    total_models = len(model_names)\n",
    "    if funcs_for_kalman:\n",
    "        total_models+=1\n",
    "\n",
    "    colormap = plt.cm.tab10 #nipy_spectral, Set1,Paired\n",
    "    colors = [colormap(i) for i in np.linspace(0, 1, 8)]\n",
    "\n",
    "    widths_for_ax = []\n",
    "    tit = \"\"\n",
    "    for idx, (model_name, model_results) in enumerate(zip(model_names, model_results_arr)):\n",
    "\n",
    "        all_tracks_df = model_results[0]\n",
    "\n",
    "        all_tracks_df['pt'] = LA.norm(all_tracks_df[['px','py']].values, axis=1)\n",
    "        all_tracks_df['pt'] = all_tracks_df['pt'].astype('float32')\n",
    "        all_tracks_df['cos_t'] = (all_tracks_df[['pz']].values/ LA.norm(all_tracks_df[['px','py','pz']].values, axis=1, keepdims=True))\n",
    "        all_tracks_df['a_phi'] = np.arctan2(all_tracks_df[['px']].values, all_tracks_df[['py']].values)\n",
    "\n",
    "        tracks_real, tracks_pred_true, mode, subtitle  = mode_func(all_tracks_df)\n",
    "\n",
    "        color_box = colors[idx]\n",
    "        color_line = colors[idx]\n",
    "        n_events = model_results_arr[0][0].event_id.nunique()\n",
    "        tit = f\"Models track {subtitle} on {n_events} events\"\n",
    "        ax1.set_title(tit)\n",
    "        ax1.set_ylabel(f'Track {subtitle}', fontsize=12)\n",
    "        #ax2.set_ylabel(f'Track {subtitle}', fontsize=12)\n",
    "        #ax3.set_ylabel(f'Track {subtitle}', fontsize=12)\n",
    "\n",
    "        widths_ax = draw_for_col(ax1, tracks_real, tracks_pred_true, 'pt', '$pt$', n_events, 350,\n",
    "                                 n_avg_ticks=48, x_ticks=20, ticks_custom=False, with_boxplot=True, model_name=model_name,\n",
    "                                 graph_idx=idx, total_indices=total_models, subtitle=subtitle,\n",
    "                                 color_line=color_line, color_box=color_box, mode=mode)\n",
    "\n",
    "        widths_for_ax.append([widths_ax])\n",
    "\n",
    "\n",
    "        widths_ax = draw_for_col(ax2, tracks_real, tracks_pred_true, 'a_phi', '$a_\\phi$',n_events, 350,\n",
    "                                 n_avg_ticks=48, x_ticks=20, with_boxplot=True, model_name=model_name,\n",
    "                                 graph_idx=idx, total_indices=total_models, subtitle=subtitle,\n",
    "                                 color_line=color_line, color_box=color_box, mode=mode)\n",
    "        widths_for_ax[-1].append(widths_ax)\n",
    "        #\n",
    "        ##enable on big dataset\n",
    "        widths_ax = draw_for_col(ax3, tracks_real, tracks_pred_true,'cos_t', '$cos_t$',n_events, 350,\n",
    "                                 n_avg_ticks=48,x_ticks=20, ticks_custom=False, with_boxplot=True, model_name=model_name,\n",
    "                                 graph_idx=idx, total_indices=total_models, subtitle=subtitle,\n",
    "                                 color_line=color_line, color_box=color_box, mode=mode)\n",
    "        widths_for_ax[-1].append(widths_ax)\n",
    "\n",
    "    if funcs_for_kalman:\n",
    "        for idx, ax in enumerate([ax1,ax2,ax3]):\n",
    "            color_box = colors[-2]\n",
    "            color_line = colors[-2]\n",
    "            funcs_for_kalman[idx](ax, widths_for_ax[-1][idx], color_box, color_line)\n",
    "\n",
    "    for ax in [ax1,ax2,ax3]:\n",
    "        ax.grid()\n",
    "        ax.legend(bbox_to_anchor=(1, -0.05))\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    #fig.legend(handles, labels, loc='lower right', bbox_to_anchor=(1, -0.05))\n",
    "    #plt.title()\n",
    "    plt.tight_layout()\n",
    "    plt.rcParams['savefig.facecolor']='white'\n",
    "\n",
    "    os.makedirs('../output', exist_ok=True)\n",
    "    #plt.savefig('../output/%s.png'%(''.join([i if (ord(i) < 128) and (ord(i)>ord('a')) else '_' for i in tit])), dpi=300)\n",
    "    #return all_tracks_df\n",
    "    plt.show()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_model_results([\"TrackNetV3\"],    #\n",
    "                   [[joined_res, ]],\n",
    "                   to_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_results([\"TrackNetV3\"],\n",
    "                   [results_tracknet],\n",
    "                   to_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_model_results([\"TrackNetV3\"],\n",
    "                   [results_tracknet],\n",
    "                   to_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ariadne_gpu_kernel",
   "language": "python",
   "name": "ariadne_gpu_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
